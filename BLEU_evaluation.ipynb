{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4is89c0DZv6"
      },
      "source": [
        "## BLEU Score Calculation to Compare Similarity Between LLM Response and Ground Truth Response from Human Coaches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Running this cell may make changes to your environment\n",
        "\n",
        "# !pip install transformers sacrebleu\n",
        "# !pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koT981asCkVP"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXCMg4aqCw9A"
      },
      "outputs": [],
      "source": [
        "# Function to calculate BLEU score for two texts\n",
        "def calculate_bleu(reference, candidate):\n",
        "    reference_tokens = [word_tokenize(reference.lower())]\n",
        "    candidate_tokens = word_tokenize(candidate.lower())\n",
        "    # Applying smoothing function for cases where perfect matches don't occur\n",
        "    smoothie = SmoothingFunction().method1\n",
        "    return sentence_bleu(reference_tokens, candidate_tokens, weights=(1, 0, 0, 0), smoothing_function=smoothie)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hLwH3sbCkVQ"
      },
      "outputs": [],
      "source": [
        "# Load our LLM responses\n",
        "with open('prompt1_LLM_response.json', 'r', encoding='utf-8') as file:\n",
        "    llm_data = json.load(file)\n",
        "\n",
        "# Load ground truth responses\n",
        "# We used testing transcripts as the ground truth to compare with LLM responses.\n",
        "# The testing transcripts are the real transcripts of leadership coaching sessions happend between a human coach and a leader\n",
        "# Therefore, the human coach responses are considered as ground truth.\n",
        "with open('Testing Transcripts.json', 'r', encoding='utf-8') as file:\n",
        "    ground_truth_data = json.load(file)\n",
        "\n",
        "filtered_gt_data = [item for item in ground_truth_data if 'coach' in item]\n",
        "\n",
        "# List to hold individual BLEU scores\n",
        "bleu_scores = []\n",
        "\n",
        "# Assuming both files are of the same length and corresponding indices match\n",
        "for llm_item, gt_item in zip(llm_data, filtered_gt_data):\n",
        "    ground_truth_response = gt_item['coach']\n",
        "    llm_response = llm_item['response']\n",
        "    score = calculate_bleu(ground_truth_response, llm_response)\n",
        "    bleu_scores.append(score)\n",
        "    print(f\"Prompt: {llm_item['prompt']}\\nGround Truth: {ground_truth_response}\\nLLM Response: {llm_response}\\nBLEU Score: {score}\\n\")\n",
        "\n",
        "# Calculate average BLEU\n",
        "average_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
        "print(f\"Average BLEU Score across all prompts: {average_bleu_score}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "68c46620193ef95b122b2567699a0018ca7cc4e872ffd9d39cfeb4c4c0222059"
    },
    "kernelspec": {
      "display_name": "Python 3.7.2 ('cse160')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
