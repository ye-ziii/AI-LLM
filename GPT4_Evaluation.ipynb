{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_hP6cM0EA58"
      },
      "source": [
        "## GPT4 Evaluation of LLM Performance to Compare Different Prompt Tip Versions or Different Models against Ground Truths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTIxK_HaD_gl"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "# from cloudgpt_aoai import  *\n",
        "import random\n",
        "import os\n",
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "km_bvRj1D_gm"
      },
      "outputs": [],
      "source": [
        "client = OpenAI(\n",
        "    api_key= API_Key,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbvZSuC44Shx"
      },
      "source": [
        "### Functions Used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TAjvNumsy0A"
      },
      "outputs": [],
      "source": [
        "def get_chat_completion(messages):\n",
        "    response = client.chat.completions.create(\n",
        "        model= \"gpt-4-turbo\",\n",
        "        messages=messages,\n",
        "        temperature=0.2,\n",
        "        # max_tokens=1200,\n",
        "        top_p=0.5,\n",
        "        frequency_penalty=0,\n",
        "        # presence_penalty=0,\n",
        "        stop=None,\n",
        "        response_format={\"type\": \"json_object\"}\n",
        "    )\n",
        "    response = response.choices[0].message.content\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAcKYWStwplz"
      },
      "outputs": [],
      "source": [
        "def convert_to_dict(output):\n",
        "    \"\"\"Convert JSON string to dictionary if necessary.\"\"\"\n",
        "    if isinstance(output, str):\n",
        "        try:\n",
        "            output = json.loads(output)\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(\"Failed to decode JSON: \", e)\n",
        "            output = {}\n",
        "    elif not isinstance(output, dict):\n",
        "        print(\"Output is neither a string nor a dictionary.\")\n",
        "        output = {}\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4LAWo7I4ZYM"
      },
      "source": [
        "### Load two competing LLM response transcripts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7GvDw3n2bVW"
      },
      "outputs": [],
      "source": [
        "# In this case, we compare two transcripts in which the coach responses are generated by the same LLM model\n",
        "# with 2 different system prompt tips to compare which prompt is more effective.\n",
        "\n",
        "# We can also compare two transcripts in which the coach responses are generated by 2 different LLMs with the\n",
        "# same prompt tips to compare model capabilities.\n",
        "with open('prompt1_LLM_response.json', 'r') as file:\n",
        "    prompt1_response = json.load(file)\n",
        "\n",
        "with open('prompt2_LLM_response.json', 'r') as file:\n",
        "    prompt2_response = json.load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZPufyXm-ePV"
      },
      "source": [
        "### Evaluation prompt tips for GPT4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82tmQeS8D_gm"
      },
      "outputs": [],
      "source": [
        "system_prompt = \"\"\"\n",
        "You are an insightful and meticulous evaluator tasked with assessing the performance of leadership coaching AI systems.\n",
        "We would like to invite you to provide feedback on the performance of two AI assistants in coaching a leader with the <Question>, compared with the <Human Coach Answer>.\n",
        "These two AI assistants answers are in <Answer1> and <Answer2>, respectively. You should not focus on the length of the answer or the details of the answer, a longer answer or answer containing more details does NOT necessarily mean the answer is better.\n",
        "Please first provide a comprehensive explanation of your evaluation, avoiding any potential bias on answer length and details and ensuring that the order in which the responses were presented does not affect your judgment. Then, output your decision indicating your preference on <Answer1> and <Answer2>.\n",
        "You are allowed to choose one of the following options: \"Answer1\", \"Answer2\", or \"Tie\". \"Answer1\" means you prefer the first answer, \"Answer2\" means you prefer the second answer, and \"Tie\" means you have no preference between the two answers.\n",
        "\n",
        "There are some Evaluation Metrics you can follow to compare \"Answer1\" and \"Answer2\":\n",
        ".....\n",
        "\n",
        "\n",
        "Your output should be in the following format in JSON:\n",
        "{{\n",
        "   \"evaluation_evidence\": \"your evaluation explanation here\",\n",
        "   \"evaluation_decision\": \"Answer1, or Answer2, or Tie\"\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "prompt= \"\"\"\n",
        "Here is the user's question <Question>: {question}\n",
        "The grounded answer <Grounded Answer>: {answer_gt}\n",
        "The first answer <Answer1> is: {answer1}\n",
        "The second answer <Answer2> is: {answer2}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVOQBM3R-k-s"
      },
      "source": [
        "### Load & Organize User Queries and Ground Truth Responses(Human Coach)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jOPaT-AD_gm"
      },
      "outputs": [],
      "source": [
        "random.seed(0)\n",
        "\n",
        "all_lines_q = []\n",
        "all_lines_a = []\n",
        "\n",
        "# load the human coach transcript as the ground truth answer\n",
        "with open('Testing Transcripts.json', 'r', encoding='utf-8') as file:\n",
        "    conversations = json.load(file)\n",
        "\n",
        "# Seperate user queries and huamn coach responses into seperate lists respectively\n",
        "for entry in conversations:\n",
        "    if 'client' in entry:\n",
        "        query = entry['client'] + '\\ncoach:\\n'\n",
        "        all_lines_q.append(query)\n",
        "    elif 'coach' in entry:\n",
        "        all_lines_a.append(entry['coach'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9eZJkrO--Ec"
      },
      "source": [
        "### Generate & Store Evaluation Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMILnVm-D_gm"
      },
      "outputs": [],
      "source": [
        "result_all = []\n",
        "tie = 0\n",
        "p2 = 0\n",
        "p1 = 0\n",
        "\n",
        "for i in range(len(all_lines_a)):\n",
        "    print(i)\n",
        "    problem = all_lines_q[i]\n",
        "    prompt1_sl = prompt1_response[i]['response']\n",
        "    gt_sl = all_lines_a[i]\n",
        "    prompt2_sl = prompt2_response[i]['response']\n",
        "\n",
        "    # use shuffling to mitigate position bias and ensure a fair comparison and more robust evaluation\n",
        "    shuffle=False\n",
        "    if random.random() > 0.5:\n",
        "        shuffle=True\n",
        "        modified_text = prompt.format(question = problem, answer_gt = gt_sl, answer1=prompt1_sl, answer2=prompt2_sl)\n",
        "    else:\n",
        "        modified_text = prompt.format(question = problem, answer_gt = gt_sl, answer1=prompt2_sl, answer2=prompt1_sl)\n",
        "\n",
        "    # generate evaluation results\n",
        "    test_chat_message = [{\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": modified_text}]\n",
        "    rre = get_chat_completion(test_chat_message)\n",
        "\n",
        "    # store evaluation results for downstream analysis\n",
        "    temp_ = {}\n",
        "    temp_['problem'] = problem\n",
        "    temp_['ground truth'] = gt_sl\n",
        "    temp_['prompt1'] = prompt1_sl\n",
        "    temp_['prompt2'] = prompt2_sl\n",
        "    temp_['shuffle'] = shuffle\n",
        "\n",
        "    result = convert_to_dict(rre)\n",
        "\n",
        "    temp_['explanation'] = result['evaluation_evidence']\n",
        "    preference=result['evaluation_decision']\n",
        "\n",
        "    if shuffle:\n",
        "        if preference.lower() == 'answer1':\n",
        "            preference = 'prompt1'\n",
        "            p1 += 1\n",
        "        elif preference.lower() == 'answer2':\n",
        "            preference = 'prompt2'\n",
        "            p2 += 1\n",
        "        else:\n",
        "            preference = \"Tie\"\n",
        "            tie += 1\n",
        "    else:\n",
        "        if preference.lower() == 'answer1':\n",
        "            preference = 'prompt2'\n",
        "            p2 += 1\n",
        "        elif preference.lower() == 'answer2':\n",
        "            preference = 'prompt1'\n",
        "            p1 += 1\n",
        "        else:\n",
        "            preference = \"Tie\"\n",
        "            tie += 1\n",
        "    temp_['preference'] = preference\n",
        "    result_all.append(temp_)\n",
        "\n",
        "with open('evaluation_prompt1_vs_prompt2.json', 'w') as file:\n",
        "    json.dump(result_all, file, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbMxM5tf_WLX"
      },
      "source": [
        "### GPT4 Voting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6t5ByJYD_gm",
        "outputId": "07104cc2-c767-45da-da8f-e6acab26c3b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4\n",
            "3\n",
            "4\n"
          ]
        }
      ],
      "source": [
        "print(p1)\n",
        "print(p2)\n",
        "print(tie)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LqSB_tnCxhG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uP2HamhHAacn"
      },
      "source": [
        "#### **Important Note: in order to compare three responses(LLM response from prompt1, LLM response from prompt2, and ground truth response), these three responses need to be generated to answer the SAME user query for each internaction. Therefore, in order for this process above to effectively work, the imported transcripts should follow the same format and include the same user queries in the right order, each followed by one type of response for each interaction. That way, we can compare the responses in parallell.**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "68c46620193ef95b122b2567699a0018ca7cc4e872ffd9d39cfeb4c4c0222059"
    },
    "kernelspec": {
      "display_name": "Python 3.7.2 ('cse160')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
