{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Engineering - LLM Generates Responses with Context from Testing Transcripts"
      ],
      "metadata": {
        "id": "hujpa-mI9TnX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bP2Z3S-Z8cCd"
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "!pip install transformers\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from transformers import GPT2Tokenizer\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSUipyAA8cCf"
      },
      "outputs": [],
      "source": [
        "client = OpenAI(\n",
        "    api_key= API_key,\n",
        ")\n",
        "\n",
        "def get_chat_completion(messages):\n",
        "    response = client.chat.completions.create(\n",
        "        model= LLM_Model,\n",
        "        messages=messages,\n",
        "        temperature=0.2,\n",
        "        # max_tokens=800,\n",
        "        top_p=0.5,\n",
        "        frequency_penalty=0,\n",
        "        # presence_penalty=0,\n",
        "        stop=None\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDRKxzvT8cCf"
      },
      "outputs": [],
      "source": [
        "# This is our prompt tip for the LLM to follow and learn how to be a professional leadership coach.\n",
        "# We use iterative prompting to test each new version of our prompt tip and see for potential improvements of the model.\n",
        "promp_emb_text = \"\"\"\n",
        "As a leadership coach, your primary role is to guide...\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sva3NF5b8cCf"
      },
      "outputs": [],
      "source": [
        "test_chat_message = [\n",
        "    {\"role\": \"system\", \"content\": promp_emb_text}\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewNMTRGr8cCf"
      },
      "outputs": [],
      "source": [
        "# Initialize the tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "max_tokens = 4096\n",
        "responses_list = []\n",
        "\n",
        "# The testing transcripts are the real transcripts of leadership coaching sessions happend between a human coach and a leader.\n",
        "# We used them as context to feed into the LLM and let it generate the next response to the leader based on all the previous interactions.\n",
        "with open('Testing Transcripts.json', 'r', encoding='utf-8') as file:\n",
        "    conversations = json.load(file)\n",
        "\n",
        "# Process each entry in the conversations\n",
        "for entry in conversations:\n",
        "        # Append both client and coach entries to the conversation history in test_chat_message\n",
        "    if 'client' in entry:\n",
        "        client_query = {'role': \"user\", \"content\": entry['client'] + \"\\nassistant:\\n\"}\n",
        "        test_chat_message.append(client_query)\n",
        "    if 'coach' in entry:\n",
        "        test_chat_message.append({'role': \"assistant\", \"content\": entry['coach']})\n",
        "\n",
        "    if 'client' in entry:\n",
        "        response = get_chat_completion(test_chat_message)\n",
        "        temp_res = {\n",
        "            'prompt': client_query['content'],  # Use the formatted client query\n",
        "            'response': response\n",
        "        }\n",
        "        responses_list.append(temp_res)\n",
        "\n",
        "# Write LLM responses to a JSON file\n",
        "with open('prompt1_test_response.json', 'w', encoding='utf-8') as file:\n",
        "    json.dump(responses_list, file, ensure_ascii=False, indent=4)\n"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "68c46620193ef95b122b2567699a0018ca7cc4e872ffd9d39cfeb4c4c0222059"
    },
    "kernelspec": {
      "display_name": "Python 3.7.2 ('cse160')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}