{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Engineering - LLM Generates Responses without Previous Interaction Context"
      ],
      "metadata": {
        "id": "tYz2Q_SbBJnG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dl4QSV-dAtzr"
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "import os\n",
        "from openai import OpenAI\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXSi0t5EAtzs"
      },
      "outputs": [],
      "source": [
        "client = OpenAI(\n",
        "    api_key= API_Key,\n",
        ")\n",
        "\n",
        "def get_chat_completion(messages):\n",
        "    response = client.chat.completions.create(\n",
        "        model= LLM_Model,\n",
        "        messages=messages,\n",
        "        temperature=0.2,\n",
        "        # max_tokens=800,\n",
        "        top_p=0.5,\n",
        "        frequency_penalty=0,\n",
        "        # presence_penalty=0,\n",
        "        stop=None\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyHbw8ljAtzs"
      },
      "outputs": [],
      "source": [
        "# This is our prompt tip for the LLM to follow and learn how to be a professional leadership coach.\n",
        "# We use iterative prompting to test each new version of our prompt tip and see for potential improvements of the model.\n",
        "promp_emb_text = \"\"\"\n",
        "As a leadership coach, your primary role is to guide...\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnB_FDczAtzs"
      },
      "outputs": [],
      "source": [
        "test_chat_message = [\n",
        "    {\"role\": \"system\", \"content\": promp_emb_text}\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VONbtH4Atzs"
      },
      "outputs": [],
      "source": [
        "responses_list = []\n",
        "\n",
        "# The testing transcripts are the real transcripts of leadership coaching sessions happend between a human coach and a leader.\n",
        "# In this case, we only feed the LLM model with the newest leader's query without previous interactions and let the LLM generate\n",
        "# the next response to the leader soly based on the single leader query each time and the system's prompt tips.\n",
        "with open('Testing Transcripts.json', 'r', encoding='utf-8') as file:\n",
        "    conversations = json.load(file)\n",
        "\n",
        "# Process each entry in the conversations\n",
        "for entry in conversations:\n",
        "    test_chat_message = [{\"role\": \"system\", \"content\": promp_emb_text}]\n",
        "\n",
        "    if 'client' in entry:\n",
        "        temp_res = {}\n",
        "        client_text = entry['client']\n",
        "        query = client_text + \"\\nassistant:\\n\"\n",
        "        test_chat_message.append({'role': \"user\", 'content': query})\n",
        "\n",
        "        response = get_chat_completion(test_chat_message)\n",
        "\n",
        "        temp_res['prompt'] = query\n",
        "        temp_res['response'] = response\n",
        "        responses_list.append(temp_res)\n",
        "\n",
        "with open('prompt1_test_response.json', 'w', encoding='utf-8') as file:\n",
        "    json.dump(responses_list, file, ensure_ascii=False, indent=4)\n"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "68c46620193ef95b122b2567699a0018ca7cc4e872ffd9d39cfeb4c4c0222059"
    },
    "kernelspec": {
      "display_name": "Python 3.7.2 ('cse160')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}